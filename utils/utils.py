import logging
from datetime import datetime
import os
import shutil
import csv
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook



# Download files from S3
def download_files_from_s3(bucket_name, file_prefix, local_path_dir, aws_conn_id, **kwargs):
    try:
        start_time = datetime.now()
        logging.info(f"Started downloading files at {start_time}")
        s3_hook = S3Hook(aws_conn_id=aws_conn_id)
        files = s3_hook.list_keys(bucket_name, prefix=file_prefix)
        
        if not files:
            logging.info(f"No files found in S3 with prefix '{file_prefix}'")
            return
        
        for file in files:
            logging.info(f"Started downloading {file} from S3 to {local_path_dir}")
            filename = file.split('/')[-1]
            local_file_path = os.path.join(local_path_dir, filename)
            if not os.path.exists(local_file_path):
                s3_hook.download_file(
                    key=file,
                    bucket_name=bucket_name,
                    preserve_file_name=True,
                    use_autogenerated_subdir=False,
                    local_path=local_path_dir,
                )
                logging.info(f"Downloaded {file} from S3 to {local_path_dir}")
            else:
                logging.info(f"File {filename} already exists at {local_path_dir}")
        end_time = datetime.now()
        logging.info(f"Finished downloading files at {end_time}, duration: {end_time - start_time}")
    except Exception as e:
        logging.error(f"Error downloading files from S3: {str(e)}")
        raise

def upload_files_to_s3(bucket_name, local_file_path, s3_key=None):
    """Upload a file to an S3 bucket."""
    if not s3_key:
        s3_key = os.path.basename(local_file_path)
    hook = S3Hook(aws_conn_id='aws_default')  # Adjust aws_conn_id as needed
    hook.load_file(filename=local_file_path, bucket_name=bucket_name, key=s3_key, replace=True)
    print(f"File uploaded to s3://{bucket_name}/{s3_key}")

# Ingest files into PostgreSQL
def ingest_files_to_postgres(local_path_dir, file_prefix, database_name, table_name, postgres_conn_id, **kwargs):
    try:
        start_time = datetime.now()
        logging.info(f"Started ingesting files at {start_time}")
        files = [f for f in os.listdir(local_path_dir) if f.startswith(file_prefix)]
        if not files:
            logging.info(f"No files found in {local_path_dir} with prefix '{file_prefix}'")
            return
        
        pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id, schema=database_name)
        
        for file in files:
            logging.info(f"Started ingesting {file} into PostgreSQL table '{table_name}'")
            file_path = os.path.join(local_path_dir, file)
            # preprocess_csv(file_path)  # Preprocess CSV to remove empty rows
            sql = f"COPY {table_name} FROM STDIN WITH CSV HEADER DELIMITER ','"
            with open(file_path, 'r') as f:
                pg_hook.copy_expert(sql=sql, filename=file_path)
            logging.info(f"Ingested {file} into PostgreSQL table '{table_name}'")
        end_time = datetime.now()
        logging.info(f"Finished ingesting files at {end_time}, duration: {end_time - start_time}")
    except Exception as e:
        logging.error(f"Error ingesting files into database: {str(e)}")
        raise


# Remove directory from local
def clean_local_files(local_path_dir, **kwargs):
    try:
        start_time = datetime.now()
        logging.info(f"Started cleaning files at {start_time}")
        shutil.rmtree(local_path_dir)
        end_time = datetime.now()
        logging.info(f"Finished cleaning files at {end_time}, duration: {end_time - start_time}")
    except Exception as e:
        logging.error(f"Error cleaning files from local directory: {str(e)}")
        raise


# Check if table is there in database or not if not then creates in database
def check_and_create_table(database_name, table_name, postgres_conn_id, create_table_sql, **kwargs):
    try:
        pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id, schema=database_name)
        conn = pg_hook.get_conn()
        cursor = conn.cursor()
        
        # Check if the table exists
        cursor.execute(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE  table_schema = 'public'
            AND    table_name   = '{table_name}'
        );
        """)
        exists = cursor.fetchone()[0] # type: ignore
        
        if not exists:
            logging.info(f"Table '{table_name}' does not exist. Creating table...")
            cursor.execute(create_table_sql) # type: ignore
            conn.commit()
            logging.info(f"Table '{table_name}' created successfully.")
        else:
            logging.info(f"Table '{table_name}' already exists. No action needed.")
    except Exception as e:
        logging.error(f"Error checking or creating table '{table_name}': {str(e)}")
        raise
    finally:
        cursor.close()
        conn.close()

# truncate table in database
def truncate_table(database_name, table_name, postgres_conn_id, **kwargs):
    """
    Truncates the specified table in the PostgreSQL database.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id, schema=database_name)
        conn = pg_hook.get_conn()
        cursor = conn.cursor()
        
        truncate_sql = f"TRUNCATE TABLE {table_name};"
        cursor.execute(truncate_sql)
        conn.commit()
        
        logging.info(f"Table {table_name} has been truncated successfully.")
        
        cursor.close()
    except Exception as e:
        logging.error(f"Error truncating table {table_name}: {str(e)}")
        raise


def preprocess_csv(file_path):
    temp_file_path = file_path + '.tmp'
    with open(file_path, 'r', newline='', encoding='utf-8') as infile, open(temp_file_path, 'w', newline='', encoding='utf-8') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        for row in reader:
            # Only write rows that are not empty
            if any(cell.strip() for cell in row):
                writer.writerow(row)
    # Replace original file with processed file
    os.replace(temp_file_path, file_path)
