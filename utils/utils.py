"""
File Operations and Database Ingestion Utility

This module provides utility functions for managing files and interacting with databases:
- Downloading files from S3
- Uploading files to S3
- Ingesting files into PostgreSQL
- Cleaning local files
- Checking and creating PostgreSQL tables
- Truncating PostgreSQL tables
- Preprocessing CSV files to remove empty rows

Dependencies:
- `airflow.providers.amazon.aws.hooks.s3.S3Hook`
- `airflow.providers.postgres.hooks.postgres.PostgresHook`
- `csv`
- `logging`
- `os`
- `shutil`
- `datetime`
"""

import csv
import logging
import os
import shutil
from datetime import datetime

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook


def download_files_from_s3(bucket_name: str, file_prefix: str, local_path_dir: str, aws_conn_id: str) -> None:
    """
    Download files from S3 to a local directory.

    :param bucket_name: Name of the S3 bucket
    :param file_prefix: Prefix of the files to download
    :param local_path_dir: Local directory to save the downloaded files
    :param aws_conn_id: Airflow connection ID for AWS
    :param kwargs: Additional arguments
    :raises Exception: If an error occurs during file download
    """
    try:
        start_time = datetime.now()
        logging.info("Started downloading files at %s", start_time)
        s3_hook = S3Hook(aws_conn_id=aws_conn_id)
        files = s3_hook.list_keys(bucket_name, prefix=file_prefix)

        if not files:
            logging.info("No files found in S3 with prefix '%s'", file_prefix)
            return

        for file in files:
            logging.info("Started downloading %s from S3 to %s",
                         file, local_path_dir)
            filename = file.split('/')[-1]
            local_file_path = os.path.join(local_path_dir, filename)
            if not os.path.exists(local_file_path):
                s3_hook.download_file(
                    key=file,
                    bucket_name=bucket_name,
                    preserve_file_name=True,
                    use_autogenerated_subdir=False,
                    local_path=local_path_dir,
                )
                logging.info("Downloaded %s from S3 to %s",
                             file, local_path_dir)
            else:
                logging.info("File %s already exists at %s",
                             filename, local_path_dir)
        end_time = datetime.now()
        logging.info("Finished downloading files at %s, duration: %s",
                     end_time, end_time - start_time)
    except Exception as e:
        logging.error("Error downloading files from S3: %s", str(e))
        raise


def upload_files_to_s3(bucket_name: str, local_file_path: str, s3_key: str = None) -> None:  # type: ignore
    """
    Upload a file to an S3 bucket.

    :param bucket_name: Name of the S3 bucket
    :param local_file_path: Path to the local file to upload
    :param s3_key: Key for the file in S3 (optional, defaults to the file name)
    :raises Exception: If an error occurs during file upload
    """
    if not s3_key:
        s3_key = os.path.basename(local_file_path)
    hook = S3Hook(aws_conn_id='aws_default')  # Adjust aws_conn_id as needed
    hook.load_file(filename=local_file_path,
                   bucket_name=bucket_name, key=s3_key, replace=True)
    logging.info("File uploaded to s3://%s/%s", bucket_name, s3_key)


def ingest_files_to_postgres(local_path_dir: str, file_prefix: str, database_name: str, table_name: str, postgres_conn_id: str) -> None:
    """
    Ingest files from a local directory into a PostgreSQL table.

    :param local_path_dir: Local directory containing files to ingest
    :param file_prefix: Prefix of the files to ingest
    :param database_name: Name of the PostgreSQL database
    :param table_name: Name of the table to ingest data into
    :param postgres_conn_id: Airflow connection ID for PostgreSQL
    :param kwargs: Additional arguments
    :raises Exception: If an error occurs during file ingestion
    """
    try:
        start_time = datetime.now()
        logging.info("Started ingesting files at %s", start_time)
        files = [f for f in os.listdir(
            local_path_dir) if f.startswith(file_prefix)]
        if not files:
            logging.info("No files found in %s with prefix '%s'",
                         local_path_dir, file_prefix)
            return

        pg_hook = PostgresHook(
            postgres_conn_id=postgres_conn_id, schema=database_name)

        for file in files:
            logging.info(
                "Started ingesting %s into PostgreSQL table '%s'", file, table_name)
            file_path = os.path.join(local_path_dir, file)
            sql = f"COPY {table_name} FROM STDIN WITH CSV HEADER DELIMITER ','"
            with open(file_path, 'r', encoding="utf-8") as f:
                pg_hook.copy_expert(sql=sql, filename=file_path)
            logging.info("Ingested %s into PostgreSQL table '%s'",
                         file, table_name)
        end_time = datetime.now()
        logging.info("Finished ingesting files at %s, duration: %s",
                     end_time, end_time - start_time)
    except Exception as e:
        logging.error("Error ingesting files into database: %s", str(e))
        raise


def clean_local_files(local_path_dir: str) -> None:
    """
    Remove a local directory and its contents.

    :param local_path_dir: Path to the local directory to remove
    :param kwargs: Additional arguments
    :raises Exception: If an error occurs during file cleanup
    """
    try:
        start_time = datetime.now()
        logging.info("Started cleaning files at %s", start_time)
        shutil.rmtree(local_path_dir)
        end_time = datetime.now()
        logging.info("Finished cleaning files at %s, duration: %s",
                     end_time, end_time - start_time)
    except Exception as e:
        logging.error("Error cleaning files from local directory: %s", str(e))
        raise


def check_and_create_table(database_name: str, table_name: str, postgres_conn_id: str, create_table_sql: str) -> None:
    """
    Check if a table exists in PostgreSQL and create it if not.

    :param database_name: Name of the PostgreSQL database
    :param table_name: Name of the table to check/create
    :param postgres_conn_id: Airflow connection ID for PostgreSQL
    :param create_table_sql: SQL statement to create the table
    :param kwargs: Additional arguments
    :raises Exception: If an error occurs during table creation
    """
    try:
        pg_hook = PostgresHook(
            postgres_conn_id=postgres_conn_id, schema=database_name)
        conn = pg_hook.get_conn()
        cursor = conn.cursor()

        cursor.execute(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public'
            AND table_name = '{table_name}'
        );
        """)
        exists = cursor.fetchone()[0]  # type: ignore

        if not exists:
            logging.info(
                "Table '%s' does not exist. Creating table...", table_name)
            cursor.execute(create_table_sql)
            conn.commit()
            logging.info("Table '%s' created successfully.", table_name)
        else:
            logging.info(
                "Table '%s' already exists. No action needed.", table_name)
    except Exception as e:
        logging.error("Error checking or creating table '%s': %s",
                      table_name, str(e))
        raise
    finally:
        cursor.close()
        conn.close()


def truncate_table(database_name: str, table_name: str, postgres_conn_id: str) -> None:
    """
    Truncate the specified table in the PostgreSQL database.

    :param database_name: Name of the PostgreSQL database
    :param table_name: Name of the table to truncate
    :param postgres_conn_id: Airflow connection ID for PostgreSQL
    :param kwargs: Additional arguments
    :raises Exception: If an error occurs during table truncation
    """
    try:
        pg_hook = PostgresHook(
            postgres_conn_id=postgres_conn_id, schema=database_name)
        conn = pg_hook.get_conn()
        cursor = conn.cursor()

        truncate_sql = f"TRUNCATE TABLE {table_name};"
        cursor.execute(truncate_sql)
        conn.commit()

        logging.info("Table '%s' has been truncated successfully.", table_name)
    except Exception as e:
        logging.error("Error truncating table '%s': %s", table_name, str(e))
        raise
    finally:
        cursor.close()
        conn.close()


def preprocess_csv(file_path: str) -> None:
    """
    Preprocess a CSV file to remove empty rows.

    :param file_path: Path to the CSV file
    :raises Exception: If an error occurs during CSV preprocessing
    """
    temp_file_path = file_path + '.tmp'
    try:
        with open(file_path, 'r', newline='', encoding='utf-8') as infile, open(temp_file_path, 'w', newline='', encoding='utf-8') as outfile:
            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            for row in reader:
                if any(cell.strip() for cell in row):
                    writer.writerow(row)
        os.replace(temp_file_path, file_path)
    except Exception as e:
        logging.error("Error preprocessing CSV file %s: %s", file_path, str(e))
        raise
